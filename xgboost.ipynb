{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train_Y= train['open_flag']\n",
    "train_X =train.drop(['row_id','user_id','grass_date','open_flag'],axis=1)\n",
    "train_X['last_open_day']=train_X['last_open_day'].replace(to_replace = 'Never open',value=-999).astype('int64')\n",
    "train_X['last_login_day']=train_X['last_login_day'].replace(to_replace = 'Never login',value=-999).astype('int64')\n",
    "train_X['last_checkout_day']=train_X['last_checkout_day'].replace(to_replace = 'Never checkout',value=-999).astype('int64')\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('users.csv')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test_X =test.drop(['row_id','user_id','grass_date'],axis=1)\n",
    "test_X['last_open_day']=test_X['last_open_day'].replace(to_replace = 'Never open',value=-999).astype('int64')\n",
    "test_X['last_login_day']=test_X['last_login_day'].replace(to_replace = 'Never login',value=-999).astype('int64')\n",
    "test_X['last_checkout_day']=test_X['last_checkout_day'].replace(to_replace = 'Never checkout',value=-999).astype('int64')\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import math\n",
    "train_x,val_x,train_y,val_y=train_test_split(train_X,train_Y,test_size=0.3)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(val_x.shape)\n",
    "print(val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#權重改變\n",
    "weight_ratio = float(len(train_y[train_y == 1]))/float(len(train_y[train_y == 0]))\n",
    "w_array = np.array([1]*train_y.shape[0])\n",
    "w_array[train_y==1] = 1/weight_ratio\n",
    "w_array[train_y==0] = 1/(1- weight_ratio)\n",
    "w_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#客製化:計算Matthews correlation coefficient\n",
    "dval=xgb.DMatrix(val_x,val_y)\n",
    "def calculate_Matthews_correlation(preds, dtrain):\n",
    "    labels = dtrain.get_label()#TRUE\n",
    "    #print(preds)#大於0.5為1的預測\n",
    "    #print(labels)\n",
    "    preds = np.array(preds)\n",
    "    preds[preds>0.5]=1\n",
    "    preds[preds<0.5]=0\n",
    "    c_matrix = sklearn.metrics.confusion_matrix(labels, preds)\n",
    "    TN = c_matrix[0,0]\n",
    "    FN = c_matrix[1,0]\n",
    "    FP = c_matrix[0,1]\n",
    "    TP = c_matrix[1,1]\n",
    "    Matthews_cor = ( (TP*TN)-(FP*FN) ) /  math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "    return 'Matthews_cor', Matthews_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#均勻表製作\n",
    "def dt(data,ori_uniform,linsp,new_lisp):\n",
    "    #new_data = 需要改變dataframe\n",
    "    #ori_uniform = 原來均勻表格式\n",
    "    #linsp = 需要改的名稱\n",
    "    #new_lsip = 新的分割\n",
    "    import copy\n",
    "    new_data = copy.deepcopy(data)\n",
    "    ##注意地點:此處len(ori_uniform[col])是要取得表中總共有幾個列(實驗)\n",
    "    for col in linsp:\n",
    "        for row in range(0,len(ori_uniform[col])):\n",
    "            new_data.loc[row,col] = new_lisp[int(data.loc[row,col]-1)]\n",
    "    return new_data\n",
    "#函數版 因使用partial 所以必須把C_函數放進來\n",
    "#此版應該可以再精簡，期待有緣人縮改\n",
    "def ug(data):\n",
    "    import copy\n",
    "    from functools import partial\n",
    "    new_data =copy.deepcopy(data) \n",
    "    def C_2(x,name):#二循環\n",
    "        if (x%2)==1:\n",
    "            x = eval(str(name+'[0]'))   \n",
    "        elif (x%2)==0:\n",
    "            x = eval(str(name+'[1]'))\n",
    "        return x\n",
    "    def C_3(x,name):#三循環\n",
    "        if (x%3)==1:\n",
    "            x = eval(str(name+'[0]'))\n",
    "        elif (x%3)==2:\n",
    "            x = eval(str(name+'[1]'))      \n",
    "        elif (x%3)==0:\n",
    "            x = eval(str(name+'[2]'))\n",
    "        return x    \n",
    "    def C_4(x,name):#四循環\n",
    "        if (x%4)==1:\n",
    "            x = eval(str(name+'[0]'))\n",
    "        \n",
    "        elif (x%4)==2:\n",
    "            x = eval(str(name+'[1]'))\n",
    "        \n",
    "        elif (x%4)==3:\n",
    "            x = eval(str(name+'[2]'))\n",
    "        \n",
    "        elif (x%4)==0:\n",
    "            x = eval(str(name+'[3]'))\n",
    "        return x\n",
    "    def C_5(x,name):#5循環\n",
    "        if (x%5)==1:\n",
    "            x = eval(str(name+'[0]'))\n",
    "        \n",
    "        elif (x%5)==2:\n",
    "            x = eval(str(name+'[1]'))\n",
    "        \n",
    "        elif (x%5)==3:\n",
    "            x = eval(str(name+'[2]'))\n",
    "        \n",
    "        elif (x%5)==4:\n",
    "            x = eval(str(name+'[3]'))\n",
    "        \n",
    "        elif (x%5)==0:\n",
    "            x = eval(str(name+'[4]'))\n",
    "        return x\n",
    "    ##注意地點:此處eval(col)是要取得原參數設定col中有幾個元素，這邊使用廣域變數取得到的\n",
    "    for ind,col in enumerate(new_data.columns):\n",
    "        if len(eval(col)) ==2:\n",
    "            C_2 = partial(C_2,name= new_data.columns[ind])\n",
    "            new_data[col] = list(map(C_2,new_data[col]))\n",
    "        elif len(eval(col)) ==3:\n",
    "            C_3 = partial(C_3,name= new_data.columns[ind])\n",
    "            new_data[col] = list(map(C_3,new_data[col]))\n",
    "        elif len(eval(col)) ==4:\n",
    "            C_4 = partial(C_4,name= new_data.columns[ind])\n",
    "            new_data[col] = list(map(C_4,new_data[col]))\n",
    "        elif len(eval(col)) ==5:\n",
    "            C_5 = partial(C_5,name= new_data.columns[ind])\n",
    "            new_data[col] = list(map(C_5,new_data[col]))      \n",
    "    return (new_data,data)#newdata =新資料,data = 原始資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###超參數預設\n",
    "learning_rate=[0.03,0.1,0.3]\n",
    "subsample=[0.7,0.9,1]\n",
    "colsample_bytree=[0.7,0.9,1]\n",
    "max_depth=[3,6,9]\n",
    "min_child_weight=[0,0.5,2]\n",
    "gamma=[0,0.3,3]\n",
    "reg_lambda=[0,0.5,3]\n",
    "###讀取均勻表U30_13\n",
    "def getuniform():\n",
    "    U30_13 = pd.read_csv('U30_13.csv',header=None)\n",
    "    U30_7 = [1,2,3,4,6,12,13]\n",
    "    U30_7 = U30_13.iloc[:,np.array(U30_7)-1]\n",
    "    return U30_7\n",
    "U30_7 = getuniform()\n",
    "###超參數配置\n",
    "columns = ['learning_rate','subsample','colsample_bytree','max_depth','min_child_weight','gamma','reg_lambda']\n",
    "U30_7.columns = columns\n",
    "#print(U30_7)\n",
    "(U30_7_param,U30_7)=ug(U30_7)\n",
    "\n",
    "\n",
    "U30_7_param['learning_rate']=U30_7['learning_rate']#設回初始\n",
    "U30_7_param['gamma']=U30_7['gamma']#設回初始\n",
    "U30_7_param['reg_lambda']=U30_7['reg_lambda']#設回初始\n",
    "U30_7_param['min_child_weight']=U30_7['min_child_weight']#設回初始\n",
    "U30_7_param=dt(U30_7_param,U30_7,linsp=['learning_rate'],new_lisp =list(np.linspace(0.01,0.03,num= 30)))\n",
    "U30_7_param=dt(U30_7_param,U30_7,linsp=['gamma'],new_lisp =list(np.linspace(0.01,5,num= 30)))\n",
    "U30_7_param=dt(U30_7_param,U30_7,linsp=['reg_lambda'],new_lisp =list(np.linspace(0.01,3,num= 30)))\n",
    "U30_7_param=dt(U30_7_param,U30_7,linsp=['min_child_weight'],new_lisp =list(np.linspace(0.01,3,num= 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根據實驗計算fitness\n",
    "for expnum in range(len(U30_7_param)):\n",
    "    exp = U30_7_param.iloc[expnum,:].to_dict()\n",
    "    param_dist = {'objective':'binary:logistic',\n",
    "         'learning_rate':exp['learning_rate'],'gamma':exp['gamma'],\n",
    "         'subsample':exp['subsample'],'colsample_bytree ':exp['colsample_bytree'],\n",
    "         'reg_lambda ':exp['reg_lambda'],'max_depth ':exp['max_depth'],\n",
    "         'min_child_weight ':exp['min_child_weight']}\n",
    "    print(f'param_dist:{param_dist}')\n",
    "    clf = xgb.XGBClassifier(n_estimators=1000,**param_dist)\n",
    "    clf.fit(train_x, train_y,\n",
    "    eval_set=[(train_x, train_y), (val_x, val_y)],\n",
    "    eval_metric=['auc','error','logloss']#預設與自訂義無法放一起\n",
    "    #eval_metric=calculate_Matthews_correlation\n",
    "    ,early_stopping_rounds=100\n",
    "    ,sample_weight=w_array\n",
    "    ,verbose=True)\n",
    "    evals_result = clf.evals_result()\n",
    "    #取驗證集的Matthews_cor\n",
    "    U30_7_param.loc[expnum,'fitness'] = evals_result['validation_1']['logloss'][-100]#根據early_stopping_rounds設置Matthews_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最佳超參數設置\n",
    "expbest = np.where(U30_7_param['fitness']==np.min(U30_7_param['fitness']))[0][0]\n",
    "exp = U30_7_param.iloc[expbest,:].to_dict()\n",
    "param_dist = {'objective':'binary:logistic',\n",
    "         'learning_rate':exp['learning_rate'],'gamma':exp['gamma'],\n",
    "         'subsample':exp['subsample'],'colsample_bytree ':exp['colsample_bytree'],\n",
    "         'reg_lambda ':exp['reg_lambda'],'max_depth ':exp['max_depth'],\n",
    "         'min_child_weight ':exp['min_child_weight']}\n",
    "print(f'param_dist:{param_dist}')\n",
    "clf = xgb.XGBClassifier(n_estimators=1000,**param_dist)\n",
    "clf.fit(train_x, train_y,\n",
    "    eval_set=[(train_x, train_y), (val_x, val_y)],\n",
    "    eval_metric=['logloss','auc','error',]#預設與自訂義無法放一起\n",
    "    #eval_metric=calculate_Matthews_correlation\n",
    "    ,early_stopping_rounds=100\n",
    "    ,sample_weight=w_array\n",
    "    ,verbose=True)\n",
    "evals_result = clf.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出损失函数的变化情况\n",
    "def plot_logloss(model):\n",
    "    results = model.evals_result_\n",
    "    #print(results)\n",
    "    epochs = len(results['validation_0']['logloss'])\n",
    "    x_axis = range(0, epochs)\n",
    "    print(f'epochs:{epochs}')\n",
    "    # plot log loss\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "    #print(results['validation_1']['logloss'])\n",
    "    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "    ax.legend()\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.title('XGDboost Log Loss')\n",
    "    plt.show()\n",
    "    # plot classification error\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "    ax.legend()\n",
    "    plt.ylabel('Classification Error')\n",
    "    plt.title('XGBoostClassification Error')\n",
    "    plt.show()\n",
    "    # plot auc\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "    ax.legend()\n",
    "    plt.ylabel('auc')\n",
    "    plt.title('XGBoostClassification auc')\n",
    "    plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "plot_logloss(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###驗證函數val_x, val_y\n",
    "val_y_predictions = clf.predict(val_x)\n",
    "val_y_predprob = clf.predict_proba(val_x)\n",
    "from sklearn import metrics\n",
    "print ('AUC: %.4f' % metrics.roc_auc_score(val_y,val_y_predprob[:,1]))\n",
    "print ('ACC: %.4f' % metrics.accuracy_score(val_y,val_y_predictions))\n",
    "print ('Recall: %.4f' % metrics.recall_score(val_y,val_y_predictions))\n",
    "print ('F1-score: %.4f' %metrics.f1_score(val_y,val_y_predictions))\n",
    "print ('Matthews_corrcoef: %.4f' %metrics.matthews_corrcoef(val_y,val_y_predictions))\n",
    "print ('Precesion: %.4f' %metrics.precision_score(val_y,val_y_predictions))\n",
    "print(metrics.confusion_matrix(val_y,val_y_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict training set:\n",
    "test_X_predictions = clf.predict(test_X)\n",
    "test_X_predprob = clf.predict_proba(test_X)\n",
    "def sub_df(clf,test_X,test):\n",
    "    test_X_predictions = clf.predict(test_X)\n",
    "    test = test.drop([col for col in test.columns if col!='row_id'],axis=1)\n",
    "    test['open_flag'] = test_X_predictions\n",
    "    test.to_csv('sub.csv',index=False)\n",
    "sub_df(clf,test_X,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model.mdl\"\n",
    "# 保存模型\n",
    "clf.get_booster().save_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chlin_gpu)",
   "language": "python",
   "name": "chlin_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
